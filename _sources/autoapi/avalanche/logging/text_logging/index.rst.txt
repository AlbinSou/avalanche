:mod:`avalanche.logging.text_logging`
=====================================

.. py:module:: avalanche.logging.text_logging


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.logging.text_logging.TextLogger



.. data:: UNSUPPORTED_TYPES
   :annotation: :Tuple[Type]

   

.. py:class:: TextLogger(file=sys.stdout)

   Bases: :class:`avalanche.logging.StrategyLogger`

   The `TextLogger` class provides logging facilities
   printed to a user specified file. The logger writes
   metric results after each training epoch, evaluation
   experience and at the end of the entire evaluation stream.

   .. note::
       To avoid an excessive amount of printed lines,
       this logger will **not** print results after
       each iteration. If the user is monitoring
       metrics which emit results after each minibatch
       (e.g., `MinibatchAccuracy`), only the last recorded
       value of such metrics will be reported at the end
       of the epoch.

   .. note::
       Since this logger works on the standard output,
       metrics producing images or more complex visualizations
       will be converted to a textual format suitable for
       console printing. You may want to add more loggers
       to your `EvaluationPlugin` to better support
       different formats.

   Creates an instance of `TextLogger` class.

   :param file: destination file to which print metrics
       (default=sys.stdout).

   .. method:: log_metric(self, metric_value: MetricValue, callback: str) -> None

      This abstract method will has to be implemented by child classes.
      This method will be invoked on each callback.
      The `callback` parameter describes the callback from which the metric
      value is coming from.

      :param metric_value: The value to be logged.
      :param callback: The name of the callback (event) from which the
          metric value was obtained.
      :return: None


   .. method:: print_current_metrics(self)


   .. method:: before_training_exp(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called before `train_exp` by the `BaseStrategy`. 


   .. method:: before_eval_exp(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called before `eval_exp` by the `BaseStrategy`. 


   .. method:: after_training_epoch(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called after `train_epoch` by the `BaseStrategy`. 


   .. method:: after_eval_exp(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called after `eval_exp` by the `BaseStrategy`. 


   .. method:: before_training(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called before `train` by the `BaseStrategy`. 


   .. method:: before_eval(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called before `eval` by the `BaseStrategy`. 


   .. method:: after_training(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called after `train` by the `BaseStrategy`. 


   .. method:: after_eval(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called after `eval` by the `BaseStrategy`. 



