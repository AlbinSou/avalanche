:mod:`avalanche.logging.interactive_logging`
============================================

.. py:module:: avalanche.logging.interactive_logging


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.logging.interactive_logging.InteractiveLogger



.. py:class:: InteractiveLogger

   Bases: :class:`avalanche.logging.TextLogger`

   The `InteractiveLogger` class provides logging facilities
   for the console standard output. The logger shows
   a progress bar during training and evaluation flows and
   interactively display metric results as soon as they
   become available. The logger writes metric results after
   each training epoch, evaluation experience and at the
   end of the entire evaluation stream.

   .. note::
       To avoid an excessive amount of printed lines,
       this logger will **not** print results after
       each iteration. If the user is monitoring
       metrics which emit results after each minibatch
       (e.g., `MinibatchAccuracy`), only the last recorded
       value of such metrics will be reported at the end
       of the epoch.

   .. note::
       Since this logger works on the standard output,
       metrics producing images or more complex visualizations
       will be converted to a textual format suitable for
       console printing. You may want to add more loggers
       to your `EvaluationPlugin` to better support
       different formats.

   Creates an instance of `TextLogger` class.

   :param file: destination file to which print metrics
       (default=sys.stdout).

   .. method:: before_training_epoch(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called before `train_epoch` by the `BaseStrategy`. 


   .. method:: after_training_epoch(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called after `train_epoch` by the `BaseStrategy`. 


   .. method:: before_eval_exp(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called before `eval_exp` by the `BaseStrategy`. 


   .. method:: after_eval_exp(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called after `eval_exp` by the `BaseStrategy`. 


   .. method:: after_training_iteration(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called after the end of a training iteration by the
      `BaseStrategy`. 


   .. method:: after_eval_iteration(self, strategy: BaseStrategy, metric_values: List['MetricValue'], **kwargs)

      Called after the end of an iteration by the
      `BaseStrategy`. 



