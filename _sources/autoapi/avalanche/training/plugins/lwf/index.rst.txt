:mod:`avalanche.training.plugins.lwf`
=====================================

.. py:module:: avalanche.training.plugins.lwf


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.training.plugins.lwf.LwFPlugin



.. py:class:: LwFPlugin(alpha=1, temperature=2)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   A Learning without Forgetting plugin.
   LwF uses distillation to regularize the current loss with soft targets
   taken from a previous version of the model.
   This plugin does not use task identities.

   :param alpha: distillation hyperparameter. It can be either a float
           number or a list containing alpha for each experience.
   :param temperature: softmax temperature for distillation

   .. method:: penalty(self, out, x, alpha)

      Compute weighted distillation loss.


   .. method:: before_backward(self, strategy, **kwargs)

      Add distillation loss


   .. method:: after_training_exp(self, strategy, **kwargs)

      Save a copy of the model after each experience



