:mod:`avalanche.training.plugins`
=================================

.. py:module:: avalanche.training.plugins


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   agem/index.rst
   cwr_star/index.rst
   evaluation/index.rst
   ewc/index.rst
   gdumb/index.rst
   gem/index.rst
   lwf/index.rst
   multi_head/index.rst
   replay/index.rst
   strategy_plugin/index.rst
   synaptic_intelligence/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.training.plugins.AGEMPlugin
   avalanche.training.plugins.CWRStarPlugin
   avalanche.training.plugins.EvaluationPlugin
   avalanche.training.plugins.EWCPlugin
   avalanche.training.plugins.GDumbPlugin
   avalanche.training.plugins.GEMPlugin
   avalanche.training.plugins.LwFPlugin
   avalanche.training.plugins.MultiHeadPlugin
   avalanche.training.plugins.ReplayPlugin
   avalanche.training.plugins.StrategyPlugin
   avalanche.training.plugins.SynapticIntelligencePlugin



.. py:class:: AGEMPlugin(patterns_per_experience: int, sample_size: int)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   Average Gradient Episodic Memory Plugin.
   AGEM projects the gradient on the current minibatch by using an external
   episodic memory of patterns from previous experiences. If the dot product
   between the current gradient and the (average) gradient of a randomly
   sampled set of memory examples is negative, the gradient is projected.
   This plugin does not use task identities.

   :param patterns_per_experience: number of patterns per experience in the
       memory.
   :param sample_size: number of patterns in memory sample when computing
       reference gradient.

   .. method:: before_training_iteration(self, strategy, **kwargs)

      Compute reference gradient on memory sample.


   .. method:: after_backward(self, strategy, **kwargs)

      Project gradient based on reference gradients


   .. method:: after_training_exp(self, strategy, **kwargs)

      Save a copy of the model after each experience


   .. method:: sample_from_memory(self, sample_size)

      Sample a minibatch from memory.
      Return a tuple of patterns (tensor), targets (tensor).


   .. method:: update_memory(self, dataloader)

      Update replay memory with patterns from current experience.



.. py:class:: CWRStarPlugin(model, cwr_layer_name=None, freeze_remaining_model=True)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   Base class for strategy plugins. Implements all the callbacks required
   by the BaseStrategy with an empty function. Subclasses must override
   the callbacks.

   CWR* Strategy.
   This plugin does not use task identities.

   :param model: the model.
   :param cwr_layer_name: name of the last fully connected layer. Defaults
       to None, which means that the plugin will attempt an automatic
       detection.
   :param freeze_remaining_model: If True, the plugin will freeze (set
       layers in eval mode and disable autograd for parameters) all the
       model except the cwr layer. Defaults to True.

   .. method:: after_training_exp(self, strategy, **kwargs)


   .. method:: before_training_exp(self, strategy, **kwargs)


   .. method:: consolidate_weights(self)

      Mean-shift for the target layer weights


   .. method:: set_consolidate_weights(self)

      set trained weights 


   .. method:: reset_weights(self, cur_clas)

      reset weights


   .. method:: get_cwr_layer(self) -> Optional[Linear]


   .. method:: freeze_other_layers(self)



.. py:class:: EvaluationPlugin(*metrics: Union['PluginMetric', Sequence['PluginMetric']], loggers: Union['StrategyLogger', Sequence['StrategyLogger']] = None, collect_all=True)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   An evaluation plugin that obtains relevant data from the
   training and eval loops of the strategy through callbacks.
   The plugin keeps a dictionary with the last recorded value for each metric.
   The dictionary will be returned by the `train` and `eval` methods of the
   strategies.
   It is also possible to keep a dictionary with all recorded metrics by
   specifying `collect_all=True`. The dictionary can be retrieved via
   the `get_all_metrics` method.

   This plugin also logs metrics using the provided loggers.

   Creates an instance of the evaluation plugin.

   :param metrics: The metrics to compute.
   :param loggers: The loggers to be used to log the metric values.
   :param collect_all: if True, collect in a separate dictionary all
       metric curves values. This dictionary is accessible with
       `get_all_metrics` method.

   .. method:: _update_metrics(self, strategy: BaseStrategy, callback: str)


   .. method:: get_last_metrics(self)

      Return dictionary with metric names as keys and last metrics
      value as values.

      :return: a dictionary with full metric
          names as keys and last metric value as value.


   .. method:: get_all_metrics(self)

      Return all collected metrics. This method should be called
      only when `collect_all` is set to True.

      :return: if `collect_all` is True, returns a dictionary
          with full metric names as keys and a tuple of two lists
          as value. The first list gathers x values (indices
          representing time steps at which the corresponding
          metric value has been emitted). The second list
          gathers metric values. a dictionary. If `collect_all`
          is False return an empty dictionary


   .. method:: reset_last_metrics(self)

      Set the dictionary storing last value for each metric to be
      empty dict.


   .. method:: before_training(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_training_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_train_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_train_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_training_epoch(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_training_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_backward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_backward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_update(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_update(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_epoch(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_iteration(self, strategy: BaseStrategy, **kwargs)



.. py:class:: EWCPlugin(ewc_lambda, mode='separate', decay_factor=None, keep_importance_data=False)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   Elastic Weight Consolidation (EWC) plugin.
   EWC computes importance of each weight at the end of training on current
   experience. During training on each minibatch, the loss is augmented
   with a penalty which keeps the value of the current weights close to the
   value they had on previous experiences in proportion to their importance
   on that experience. Importances are computed with an additional pass on the
   training set. This plugin does not use task identities.

   :param ewc_lambda: hyperparameter to weigh the penalty inside the total
          loss. The larger the lambda, the larger the regularization.
   :param mode: `separate` to keep a separate penalty for each previous
          experience.
          `online` to keep a single penalty summed with a decay factor
          over all previous tasks.
   :param decay_factor: used only if mode is `online`.
          It specifies the decay term of the importance matrix.
   :param keep_importance_data: if True, keep in memory both parameter
           values and importances for all previous task, for all modes.
           If False, keep only last parameter values and importances.
           If mode is `separate`, the value of `keep_importance_data` is
           set to be True.

   .. method:: before_backward(self, strategy, **kwargs)

      Compute EWC penalty and add it to the loss.


   .. method:: after_training_exp(self, strategy, **kwargs)

      Compute importances of parameters after each experience.


   .. method:: compute_importances(self, model, criterion, optimizer, dataset, device, batch_size)

      Compute EWC importance matrix for each parameter


   .. method:: update_importances(self, importances, t)

      Update importance for each parameter based on the currently computed
      importances.



.. py:class:: GDumbPlugin(mem_size=200)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   A GDumb plugin. At each experience the model
   is trained with all and only the data of the external memory.
   The memory is updated at the end of each experience to add new classes or
   new examples of already encountered classes.
   In multitask scenarios, mem_size is the memory size for each task.
   This plugin can be combined with a Naive strategy to obtain the
   standard GDumb strategy.
   https://www.robots.ox.ac.uk/~tvg/publications/2020/gdumb.pdf

   .. method:: after_train_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)

      Before training we make sure to organize the memory following
      GDumb approach and updating the dataset accordingly.



.. py:class:: GEMPlugin(patterns_per_experience: int, memory_strength: float)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   Gradient Episodic Memory Plugin.
   GEM projects the gradient on the current minibatch by using an external
   episodic memory of patterns from previous experiences. The gradient on
   the current minibatch is projected so that the dot product with all the
   reference gradients of previous tasks remains positive.
   This plugin does not use task identities.

   :param patterns_per_experience: number of patterns per experience in the
       memory.
   :param memory_strength: offset to add to the projection direction
       in order to favour backward transfer (gamma in original paper).

   .. method:: before_training_iteration(self, strategy, **kwargs)

      Compute gradient constraints on previous memory samples from all
      experiences.


   .. method:: after_backward(self, strategy, **kwargs)

      Project gradient based on reference gradients


   .. method:: after_training_exp(self, strategy, **kwargs)

      Save a copy of the model after each experience


   .. method:: update_memory(self, dataset, t, batch_size)

      Update replay memory with patterns from current experience.


   .. method:: solve_quadprog(self, g)

      Solve quadratic programming with current gradient g and
      gradients matrix on previous tasks G.
      Taken from original code:
      https://github.com/facebookresearch/GradientEpisodicMemory/blob/master/model/gem.py



.. py:class:: LwFPlugin(alpha=1, temperature=2)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   A Learning without Forgetting plugin.
   LwF uses distillation to regularize the current loss with soft targets
   taken from a previous version of the model.
   This plugin does not use task identities.

   :param alpha: distillation hyperparameter. It can be either a float
           number or a list containing alpha for each experience.
   :param temperature: softmax temperature for distillation

   .. method:: _distillation_loss(self, out, prev_out)

      Compute distillation loss between output of the current model and
      and output of the previous (saved) model.


   .. method:: penalty(self, out, x, alpha)

      Compute weighted distillation loss.


   .. method:: before_backward(self, strategy, **kwargs)

      Add distillation loss


   .. method:: after_training_exp(self, strategy, **kwargs)

      Save a copy of the model after each experience



.. py:class:: MultiHeadPlugin(model, classifier_field: str = 'classifier', keep_initial_layer=False)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   Base class for strategy plugins. Implements all the callbacks required
   by the BaseStrategy with an empty function. Subclasses must override
   the callbacks.

   MultiHeadPlugin manages a multi-head readout for multi-task
   scenarios and single-head adaptation for incremental tasks.
   The plugin automatically set the correct output head when the task
   changes and adds new heads when a novel task is encountered.
   This plugin *needs task identities* for multi-task scenarios.
   It does not need task identities for single incremental tasks
   (e.g. class incremental).

   By default, a Linear (fully connected) layer is created
   with as many output units as the number of classes in that task. This
   behaviour can be changed by overriding the "create_task_layer" method.

   By default, weights are initialized using the Linear class default
   initialization. This behaviour can be changed by overriding the
   "initialize_new_task_layer" method.

   When dealing with a Single-Incremental-Task scenario, the final layer
   may get dynamically expanded. By default, the initialization provided by
   the Linear class is used and then weights of already existing classes
   are copied (that  is, without adapting the weights of new classes).
   The user can control how the new weights are initialized by overriding
   "initialize_dynamically_expanded_head".

   :param model: PyTorch model
   :param classifier_field: field of the last layer of model.
   :param keep_initial_layer: if True keeps the initial layer for task 0.

   .. method:: before_training_iteration(self, strategy, **kwargs)


   .. method:: before_eval_iteration(self, strategy, **kwargs)


   .. method:: set_task_layer(self, strategy, experience: Experience)

      Sets the correct task layer. Creates a new head for previously
      unseen tasks.

      :param strategy: the CL strategy.
      :param experience: the experience info object.
      :return: None


   .. method:: create_task_layer(self, n_output_units: int, previous_task_layer=None)

      Creates a new task layer.

      By default, this method will create a new :class:`Linear` layer with
      n_output_units" output units. If  "previous_task_layer" is None,
      the name of the classifier field is used to retrieve the amount of
      input features.

      This method will also be used to create a new layer when expanding
      an existing task head.

      This method can be overridden by the user so that a layer different
      from :class:`Linear` can be created.

      :param n_output_units: The number of output units.
      :param previous_task_layer: If not None, the previously created layer
           for the same task.
      :return: The new layer.


   .. method:: initialize_new_task_layer(self, new_layer: Module)

      Initializes a new head.

      This usually is just a weight initialization procedure, but more
      complex operations can be done as well.

      The head can be either a new layer created for a previously
      unseen task or a layer created to expand an existing task layer. In the
      latter case, the user can define a specific weight initialization
      procedure for the expanded part of the head by overriding the
      "initialize_dynamically_expanded_head" method.

      By default, if no custom implementation is provided, no specific
      initialization is done, which means that the default initialization
      provided by the :class:`Linear` class is used.

      :param new_layer: The new layer to adapt.
      :return: None


   .. method:: initialize_dynamically_expanded_head(self, prev_task_layer, new_task_layer)

      Initializes head weights for enw classes.

      This function is called by "adapt_task_layer" only.

      Defaults to no-op, which uses the initialization provided
      by "initialize_new_task_layer" (already called by "adapt_task_layer").

      This method should initialize the weights for new classes. However,
      if the strategy dictates it, this may be the perfect place to adapt
      weights of previous classes, too.

      :param prev_task_layer: New previous, not expanded, task layer.
      :param new_task_layer: The new task layer, with weights from already
          existing classes already set.
      :return:


   .. method:: adapt_task_layer(self, prev_task_layer, new_task_layer)

      Adapts the task layer by copying previous weights to the new layer and
      by calling "initialize_dynamically_expanded_head".

      This method is called by "expand_task_layer" only if a new task layer
      was created as the result of encountering a new class for that task.

      :param prev_task_layer: The previous task later.
      :param new_task_layer: The new task layer.
      :return: None.


   .. method:: expand_task_layer(self, strategy, min_n_output_units: int, task_layer)

      Expands an existing task layer.

      This method checks if the layer for a task should be expanded to
      accommodate for "min_n_output_units" output units. If the task layer
      already contains a sufficient amount of output units, no operations are
      done and "task_layer" will be returned as-is.

      If an expansion is needed, "create_task_layer" will be used to create
      a new layer and then "adapt_task_layer" will be called to copy the
      weights of already seen classes and to initialize the weights
      for the expanded part of the layer.

      :param strategy: CL strategy.
      :param min_n_output_units: The number of required output units.
      :param task_layer: The previous task layer.

      :return: The new layer for the task.



.. py:class:: ReplayPlugin(mem_size=200)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   Experience replay plugin.

   Handles an external memory filled with randomly selected
   patterns and implementing `before_training_exp` and `after_training_exp`
   callbacks. 
   The `before_training_exp` callback is implemented in order to use the
   dataloader that creates mini-batches with examples from both training
   data and external memory. The examples in the mini-batch is balanced 
   such that there are the same number of examples for each experience.    

   The `after_training_exp` callback is implemented in order to add new 
   patterns to the external memory.

   The :mem_size: attribute controls the total number of patterns to be stored 
   in the external memory.

   .. method:: before_training_exp(self, strategy, num_workers=0, shuffle=True, **kwargs)

      Dataloader to build batches containing examples from both memories and
      the training dataset


   .. method:: after_training_exp(self, strategy, **kwargs)

      After training we update the external memory with the patterns of
      the current training batch/task, adding the patterns from the new
      experience and removing those from past experiences to comply the limit
      of the total number of patterns in memory 



.. py:class:: StrategyPlugin

   Bases: :class:`StrategyCallbacks[Any]`

   Base class for strategy plugins. Implements all the callbacks required
   by the BaseStrategy with an empty function. Subclasses must override
   the callbacks.

   .. method:: before_training(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_training_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_train_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_train_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_training_epoch(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_training_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_backward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_backward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_update(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_update(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_epoch(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_dataset_adaptation(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_iteration(self, strategy: BaseStrategy, **kwargs)



.. py:class:: SynapticIntelligencePlugin(si_lambda: float, excluded_parameters: Sequence['str'] = None, device: Any = 'as_strategy')

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   The Synaptic Intelligence plugin.

   This is the Synaptic Intelligence PyTorch implementation of the
   algorithm described in the paper "Continual Learning Through Synaptic
   Intelligence" (https://arxiv.org/abs/1703.04200).

   This plugin can be attached to existing strategies to achieve a
   regularization effect.

   This plugin will require the strategy `loss` field to be set before the
   `before_backward` callback is invoked. The loss Tensor will be updated to
   achieve the S.I. regularization effect.

   Creates an instance of the Synaptic Intelligence plugin.

   :param si_lambda: Synaptic Intelligence lambda term.
   :param device: The device to use to run the S.I. experiences.
       Defaults to "as_strategy", which means that the `device` field of
       the strategy will be used. Using a different device may lead to a
       performance drop due to the required data transfer.

   .. attribute:: ewc_data
      :annotation: :EwcDataType

      The first dictionary contains the params at loss minimum while the 
      second one contains the parameter importance.


   .. method:: before_training_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_backward(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_training_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: device(self, strategy: BaseStrategy)


   .. method:: create_syn_data(model: Module, ewc_data: EwcDataType, syn_data: SynDataType, excluded_parameters: Set[str])
      :staticmethod:


   .. method:: _zero(param: Tensor)
      :staticmethod:


   .. method:: extract_weights(model: Module, target: ParamDict, excluded_parameters: Set[str])
      :staticmethod:


   .. method:: extract_grad(model, target: ParamDict, excluded_parameters: Set[str])
      :staticmethod:


   .. method:: init_batch(model, ewc_data: EwcDataType, syn_data: SynDataType, excluded_parameters: Set[str])
      :staticmethod:


   .. method:: pre_update(model, syn_data: SynDataType, excluded_parameters: Set[str])
      :staticmethod:


   .. method:: post_update(model, syn_data: SynDataType, excluded_parameters: Set[str])
      :staticmethod:


   .. method:: compute_ewc_loss(model, ewc_data: EwcDataType, excluded_parameters: Set[str], device, lambd=0.0)
      :staticmethod:


   .. method:: update_ewc_data(net, ewc_data: EwcDataType, syn_data: SynDataType, clip_to: float, excluded_parameters: Set[str], c=0.0015)
      :staticmethod:


   .. method:: explode_excluded_parameters(excluded: Set[str]) -> Set[str]
      :staticmethod:

      Explodes a list of excluded parameters by adding a generic final ".*"
      wildcard at its end.

      :param excluded: The original set of excluded parameters.

      :return: The set of excluded parameters in which ".*" patterns have been
          added.


   .. method:: not_excluded_parameters(model: Module, excluded_parameters: Set[str]) -> List[Tuple[str, Tensor]]
      :staticmethod:


   .. method:: allowed_parameters(model: Module, excluded_parameters: Set[str]) -> List[Tuple[str, Tensor]]
      :staticmethod:



