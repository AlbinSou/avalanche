:mod:`avalanche.evaluation.metrics.loss`
========================================

.. py:module:: avalanche.evaluation.metrics.loss


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.loss.Loss
   avalanche.evaluation.metrics.loss.MinibatchLoss
   avalanche.evaluation.metrics.loss.EpochLoss
   avalanche.evaluation.metrics.loss.RunningEpochLoss
   avalanche.evaluation.metrics.loss.ExperienceLoss
   avalanche.evaluation.metrics.loss.StreamLoss



Functions
~~~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.loss.loss_metrics


.. py:class:: Loss

   Bases: :class:`Metric[float]`

   The standalone Loss metric. This is a general metric
   used to compute more specific ones.

   Instances of this metric keeps the running average loss
   over multiple <prediction, target> pairs of Tensors,
   provided incrementally.
   The "prediction" and "target" tensors may contain plain labels or
   one-hot/logit vectors.

   Each time `result` is called, this metric emits the average loss
   across all predictions made since the last `reset`.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return a loss value of 0.

   Creates an instance of the loss metric.

   By default this metric in its initial state will return a loss
   value of 0. The metric can be updated by using the `update` method
   while the running loss can be retrieved using the `result` method.

   .. method:: update(self, loss: Tensor, patterns: int, task_label: int) -> None

      Update the running loss given the loss Tensor and the minibatch size.

      :param loss: The loss Tensor. Different reduction types don't affect
          the result.
      :param patterns: The number of patterns in the minibatch.
      :param task_label: the task label associated to the current experience
      :return: None.


   .. method:: result(self, task_label=None) -> Dict[int, float]

      Retrieves the running average loss per pattern.

      Calling this method will not change the internal state of the metric.
      :param task_label: None to return metric values for all the task labels.
          If an int, return value only for that task label
      :return: The running loss, as a float.


   .. method:: reset(self, task_label=None) -> None

      Resets the metric.

      :param task_label: None to reset all metric values. If an int,
          reset metric value corresponding to that task label.
      :return: None.



.. py:class:: MinibatchLoss

   Bases: :class:`avalanche.evaluation.metrics.loss.LossPluginMetric`

   The minibatch loss metric.
   This plugin metric only works at training time.

   This metric computes the average loss over patterns
   from a single minibatch.
   It reports the result after each iteration.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochLoss` instead.

   Creates an instance of the MinibatchLoss metric.

   .. method:: __str__(self)



.. py:class:: EpochLoss

   Bases: :class:`avalanche.evaluation.metrics.loss.LossPluginMetric`

   The average loss over a single training epoch.
   This plugin metric only works at training time.

   The loss will be logged after each training epoch by computing
   the loss on the predicted patterns during the epoch divided by
   the overall number of patterns encountered in that epoch.

   Creates an instance of the EpochLoss metric.

   .. method:: __str__(self)



.. py:class:: RunningEpochLoss

   Bases: :class:`avalanche.evaluation.metrics.loss.LossPluginMetric`

   The average loss across all minibatches up to the current
   epoch iteration.
   This plugin metric only works at training time.

   At each iteration, this metric logs the loss averaged over all patterns
   seen so far in the current epoch.
   The metric resets its state after each training epoch.

   Creates an instance of the RunningEpochLoss metric.

   .. method:: __str__(self)



.. py:class:: ExperienceLoss

   Bases: :class:`avalanche.evaluation.metrics.loss.LossPluginMetric`

   At the end of each experience, this metric reports
   the average loss over all patterns seen in that experience.
   This plugin metric only works at eval time.

   Creates an instance of ExperienceLoss metric

   .. method:: __str__(self)



.. py:class:: StreamLoss

   Bases: :class:`avalanche.evaluation.metrics.loss.LossPluginMetric`

   At the end of the entire stream of experiences, this metric reports the
   average loss over all patterns seen in all experiences.
   This plugin metric only works at eval time.

   Creates an instance of StreamLoss metric

   .. method:: __str__(self)



.. function:: loss_metrics(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param minibatch: If True, will return a metric able to log
       the minibatch loss at training time.
   :param epoch: If True, will return a metric able to log
       the epoch loss at training time.
   :param epoch_running: If True, will return a metric able to log
       the running epoch loss at training time.
   :param experience: If True, will return a metric able to log
       the loss on each evaluation experience.
   :param stream: If True, will return a metric able to log
       the loss averaged over the entire evaluation stream of experiences.

   :return: A list of plugin metrics.


