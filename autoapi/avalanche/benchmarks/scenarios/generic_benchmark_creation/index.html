

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>avalanche.benchmarks.scenarios.generic_benchmark_creation &mdash; Avalanche 0.1 documentation</title>
  

  
  <link rel="stylesheet" href="../../../../../_static/css/mystyle.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/graphviz.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../../genindex/" />
    <link rel="search" title="Search" href="../../../../../search/" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../../">
          

          
            
            <img src="../../../../../_static/avalanche_logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Avalanche API:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../../../">API Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../../"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche</span></code></a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../../../#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../../"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.benchmarks</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../evaluation/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../logging/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.logging</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../models/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.models</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../training/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.training</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../#submodules">Submodules</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../core/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.core</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../#package-contents">Package Contents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../#avalanche.__version__">__version__</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../">Avalanche</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../" class="icon icon-home"></a> &raquo;</li>
        
      <li><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.benchmarks.scenarios.generic_benchmark_creation</span></code></li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../../_sources/autoapi/avalanche/benchmarks/scenarios/generic_benchmark_creation/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-avalanche.benchmarks.scenarios.generic_benchmark_creation">
<span id="avalanche-benchmarks-scenarios-generic-benchmark-creation"></span><h1><a class="reference internal" href="#module-avalanche.benchmarks.scenarios.generic_benchmark_creation" title="avalanche.benchmarks.scenarios.generic_benchmark_creation"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.benchmarks.scenarios.generic_benchmark_creation</span></code></a><a class="headerlink" href="#module-avalanche.benchmarks.scenarios.generic_benchmark_creation" title="Permalink to this headline">¶</a></h1>
<p>This module contains mid-level benchmark generators.
Consider using the higher-level ones found in benchmark_generators. If none of
them fit your needs, then the helper functions here listed may help.</p>
<div class="section" id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Permalink to this headline">¶</a></h2>
<div class="section" id="functions">
<h3>Functions<a class="headerlink" href="#functions" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_multi_dataset_generic_benchmark" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_multi_dataset_generic_benchmark"><code class="xref py py-obj docutils literal notranslate"><span class="pre">create_multi_dataset_generic_benchmark</span></code></a>(train_datasets: Sequence[SupportedDataset], test_datasets: Sequence[SupportedDataset], *, other_streams_datasets: Dict[str, Sequence[SupportedDataset]] = None, complete_test_set_only: bool = False, train_transform=None, train_target_transform=None, eval_transform=None, eval_target_transform=None, other_streams_transforms: Dict[str, Tuple[Any, Any]] = None, dataset_type: AvalancheDatasetType = None) → GenericCLScenario</p></td>
<td><p>Creates a benchmark instance given a list of datasets. Each dataset will be</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists"><code class="xref py py-obj docutils literal notranslate"><span class="pre">create_generic_benchmark_from_filelists</span></code></a>(root: Optional[Union[str, Path]], train_file_lists: Sequence[Union[str, Path]], test_file_lists: Sequence[Union[str, Path]], *, other_streams_file_lists: Dict[str, Sequence[Union[str, Path]]] = None, task_labels: Sequence[int], complete_test_set_only: bool = False, train_transform=None, train_target_transform=None, eval_transform=None, eval_target_transform=None, other_streams_transforms: Dict[str, Tuple[Any, Any]] = None) → GenericCLScenario</p></td>
<td><p>Creates a benchmark instance given a list of filelists and the respective</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_paths" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_paths"><code class="xref py py-obj docutils literal notranslate"><span class="pre">create_generic_benchmark_from_paths</span></code></a>(train_lists_of_files: Sequence[Sequence[FileAndLabel]], test_lists_of_files: Union[Sequence[FileAndLabel], Sequence[Sequence[FileAndLabel]]], *, other_streams_lists_of_files: Dict[str, Sequence[Sequence[FileAndLabel]]] = None, task_labels: Sequence[int], complete_test_set_only: bool = False, train_transform=None, train_target_transform=None, eval_transform=None, eval_target_transform=None, other_streams_transforms: Dict[str, Tuple[Any, Any]] = None, dataset_type: AvalancheDatasetType = AvalancheDatasetType.UNDEFINED) → GenericCLScenario</p></td>
<td><p>Creates a benchmark instance given a sequence of lists of files. A separate</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_tensor_lists" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_tensor_lists"><code class="xref py py-obj docutils literal notranslate"><span class="pre">create_generic_benchmark_from_tensor_lists</span></code></a>(train_tensors: Sequence[Sequence[Any]], test_tensors: Sequence[Sequence[Any]], *, other_streams_tensors: Dict[str, Sequence[Sequence[Any]]] = None, task_labels: Sequence[int], complete_test_set_only: bool = False, train_transform=None, train_target_transform=None, eval_transform=None, eval_target_transform=None, other_streams_transforms: Dict[str, Tuple[Any, Any]] = None, dataset_type: AvalancheDatasetType = None) → GenericCLScenario</p></td>
<td><p>Creates a benchmark instance given lists of Tensors. A separate dataset will</p></td>
</tr>
</tbody>
</table>
<dl class="py function">
<dt id="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_multi_dataset_generic_benchmark">
<code class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.scenarios.generic_benchmark_creation.</span></code><code class="sig-name descname"><span class="pre">create_multi_dataset_generic_benchmark</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_datasets</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_datasets</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_datasets</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">SupportedDataset</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complete_test_set_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_transforms</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">AvalancheDatasetType</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">GenericCLScenario</span><a class="reference internal" href="../../../../../_modules/avalanche/benchmarks/scenarios/generic_benchmark_creation/#create_multi_dataset_generic_benchmark"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_multi_dataset_generic_benchmark" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a benchmark instance given a list of datasets. Each dataset will be
considered as a separate experience.</p>
<p>Contents of the datasets must already be set, including task labels.
Transformations will be applied if defined.</p>
<p>This function allows for the creation of custom streams as well.
While “train” and “test” datasets must always be set, the experience list
for other streams can be defined by using the <cite>other_streams_datasets</cite>
parameter.</p>
<p>If transformations are defined, they will be applied to the datasets
of the related stream.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_datasets</strong> – A list of training datasets.</p></li>
<li><p><strong>test_datasets</strong> – A list of test datasets.</p></li>
<li><p><strong>other_streams_datasets</strong> – A dictionary describing the content of custom
streams. Keys must be valid stream names (letters and numbers,
not starting with a number) while the value must be a list of dataset.
If this dictionary contains the definition for “train” or “test”
streams then those definition will override the <cite>train_datasets</cite> and
<cite>test_datasets</cite> parameters.</p></li>
<li><p><strong>complete_test_set_only</strong> – If True, only the complete test set will
be returned by the scenario. This means that the <code class="docutils literal notranslate"><span class="pre">test_dataset_list</span></code>
parameter must be list with a single element (the complete test set).
Defaults to False, which means that <code class="docutils literal notranslate"><span class="pre">train_dataset_list</span></code> and
<code class="docutils literal notranslate"><span class="pre">test_dataset_list</span></code> must contain the same amount of datasets.</p></li>
<li><p><strong>train_transform</strong> – The transformation to apply to the training data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>train_target_transform</strong> – The transformation to apply to training
patterns targets. Defaults to None.</p></li>
<li><p><strong>eval_transform</strong> – The transformation to apply to the test data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>eval_target_transform</strong> – The transformation to apply to test
patterns targets. Defaults to None.</p></li>
<li><p><strong>other_streams_transforms</strong> – Transformations to apply to custom
streams. If no transformations are defined for a custom stream,
then “train” transformations will be used. This parameter must be a
dictionary mapping stream names to transformations. The transformations
must be a two elements tuple where the first element defines the
X transformation while the second element is the Y transformation.
Those elements can be None. If this dictionary contains the
transformations for “train” or “test” streams then those transformations
will override the <cite>train_transform</cite>, <cite>train_target_transform</cite>,
<cite>eval_transform</cite> and <cite>eval_target_transform</cite> parameters.</p></li>
<li><p><strong>dataset_type</strong> – The type of the dataset. Defaults to None, which
means that the type will be obtained from the input datasets. If input
datasets are not instances of <code class="xref py py-class docutils literal notranslate"><span class="pre">AvalancheDataset</span></code>, the type
UNDEFINED will be used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">GenericCLScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists">
<code class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.scenarios.generic_benchmark_creation.</span></code><code class="sig-name descname"><span class="pre">create_generic_benchmark_from_filelists</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">root</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Path</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_file_lists</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Path</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_file_lists</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Path</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_file_lists</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Path</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complete_test_set_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_transforms</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">GenericCLScenario</span><a class="reference internal" href="../../../../../_modules/avalanche/benchmarks/scenarios/generic_benchmark_creation/#create_generic_benchmark_from_filelists"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a benchmark instance given a list of filelists and the respective
task labels. A separate dataset will be created for each filelist and each
of those datasets will be considered a separate experience.</p>
<p>This helper functions is the best shot when loading Caffe-style dataset
based on filelists.</p>
<p>Beware that this helper function is limited is the following two aspects:</p>
<ul class="simple">
<li><p>The resulting benchmark instance and the intermediate datasets used to
populate it will be of type CLASSIFICATION. There is no way to change
this.</p></li>
<li><p>Task labels can only be defined by choosing a single task label for
each experience (the same task label is applied to all patterns of
experiences sharing the same position in different streams).</p></li>
</ul>
<p>Despite those constraints, this helper function is usually sufficiently
powerful to cover most continual learning benchmarks based on file lists.</p>
<p>When in need to create a similar benchmark instance starting from an
in-memory list of paths, then the similar helper function
<a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_paths" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_paths"><code class="xref py py-func docutils literal notranslate"><span class="pre">create_generic_benchmark_from_paths()</span></code></a> can be used.</p>
<p>When in need to create a benchmark instance in which task labels are defined
in a more fine-grained way, then consider using
<a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_multi_dataset_generic_benchmark" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_multi_dataset_generic_benchmark"><code class="xref py py-func docutils literal notranslate"><span class="pre">create_multi_dataset_generic_benchmark()</span></code></a> by passing properly
initialized <code class="xref py py-class docutils literal notranslate"><span class="pre">AvalancheDataset</span></code> instances.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> – The root path of the dataset. Can be None.</p></li>
<li><p><strong>train_file_lists</strong> – A list of filelists describing the
paths of the training patterns for each experience.</p></li>
<li><p><strong>test_file_lists</strong> – A list of filelists describing the
paths of the test patterns for each experience.</p></li>
<li><p><strong>other_streams_file_lists</strong> – A dictionary describing the content of
custom streams. Keys must be valid stream names (letters and numbers,
not starting with a number) while the value must be a list of filelists
(same as <cite>train_file_lists</cite> and <cite>test_file_lists</cite> parameters). If this
dictionary contains the definition for “train” or “test” streams then
those definition will  override the <cite>train_file_lists</cite> and
<cite>test_file_lists</cite> parameters.</p></li>
<li><p><strong>task_labels</strong> – A list of task labels. Must contain at least a value
for each experience. Each value describes the task label that will be
applied to all patterns of a certain experience. For more info on that,
see the function description.</p></li>
<li><p><strong>complete_test_set_only</strong> – If True, only the complete test set will
be returned by the scenario. This means that the <code class="docutils literal notranslate"><span class="pre">test_file_lists</span></code>
parameter must be list with a single element (the complete test set).
Alternatively, can be a plain string or <code class="xref py py-class docutils literal notranslate"><span class="pre">Path</span></code> object.
Defaults to False, which means that <code class="docutils literal notranslate"><span class="pre">train_file_lists</span></code> and
<code class="docutils literal notranslate"><span class="pre">test_file_lists</span></code> must contain the same amount of filelists paths.</p></li>
<li><p><strong>train_transform</strong> – The transformation to apply to the training data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>train_target_transform</strong> – The transformation to apply to training
patterns targets. Defaults to None.</p></li>
<li><p><strong>eval_transform</strong> – The transformation to apply to the test data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>eval_target_transform</strong> – The transformation to apply to test
patterns targets. Defaults to None.</p></li>
<li><p><strong>other_streams_transforms</strong> – Transformations to apply to custom
streams. If no transformations are defined for a custom stream,
then “train” transformations will be used. This parameter must be a
dictionary mapping stream names to transformations. The transformations
must be a two elements tuple where the first element defines the
X transformation while the second element is the Y transformation.
Those elements can be None. If this dictionary contains the
transformations for “train” or “test” streams then those transformations
will override the <cite>train_transform</cite>, <cite>train_target_transform</cite>,
<cite>eval_transform</cite> and <cite>eval_target_transform</cite> parameters.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">GenericCLScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_paths">
<code class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.scenarios.generic_benchmark_creation.</span></code><code class="sig-name descname"><span class="pre">create_generic_benchmark_from_paths</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_lists_of_files</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">FileAndLabel</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_lists_of_files</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">FileAndLabel</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">FileAndLabel</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_lists_of_files</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">FileAndLabel</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complete_test_set_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_transforms</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">AvalancheDatasetType</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">AvalancheDatasetType.UNDEFINED</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">GenericCLScenario</span><a class="reference internal" href="../../../../../_modules/avalanche/benchmarks/scenarios/generic_benchmark_creation/#create_generic_benchmark_from_paths"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_paths" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a benchmark instance given a sequence of lists of files. A separate
dataset will be created for each list. Each of those datasets
will be considered a separate experience.</p>
<p>This is very similar to <a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists"><code class="xref py py-func docutils literal notranslate"><span class="pre">create_generic_benchmark_from_filelists()</span></code></a>,
with the main difference being that
<a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists"><code class="xref py py-func docutils literal notranslate"><span class="pre">create_generic_benchmark_from_filelists()</span></code></a> accepts, for each
experience, a file list formatted in Caffe-style. On the contrary, this
accepts a list of tuples where each tuple contains two elements: the full
path to the pattern and its label. Optionally, the tuple may contain a third
element describing the bounding box of the element to crop. This last
bounding box may be useful when trying to extract the part of the image
depicting the desired element.</p>
<p>Apart from that, the same limitations of
<a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_filelists"><code class="xref py py-func docutils literal notranslate"><span class="pre">create_generic_benchmark_from_filelists()</span></code></a> regarding task labels apply.</p>
<p>The label of each pattern doesn’t have to be an int. Also, a dataset type
can be defined.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_lists_of_files</strong> – A list of lists. Each list describes the paths
and labels of patterns to include in that training experience, as
tuples. Each tuple must contain two elements: the full path to the
pattern and its class label. Optionally, the tuple may contain a
third element describing the bounding box to use for cropping (top,
left, height, width).</p></li>
<li><p><strong>test_lists_of_files</strong> – A list of lists. Each list describes the paths
and labels of patterns to include in that test experience, as tuples.
Each tuple must contain two elements: the full path to the pattern
and its class label. Optionally, the tuple may contain a third element
describing the bounding box to use for cropping (top, left, height,
width).</p></li>
<li><p><strong>other_streams_lists_of_files</strong> – A dictionary describing the content of
custom streams. Keys must be valid stream names (letters and numbers,
not starting with a number) while the value follow the same structure
of <cite>train_lists_of_files</cite> and <cite>test_lists_of_files</cite> parameters. If this
dictionary contains the definition for “train” or “test” streams then
those definition will  override the <cite>train_lists_of_files</cite> and
<cite>test_lists_of_files</cite> parameters.</p></li>
<li><p><strong>task_labels</strong> – A list of task labels. Must contain at least a value
for each experience. Each value describes the task label that will be
applied to all patterns of a certain experience. For more info on that,
see the function description.</p></li>
<li><p><strong>complete_test_set_only</strong> – If True, only the complete test set will
be returned by the scenario. This means that the <code class="docutils literal notranslate"><span class="pre">test_list_of_files</span></code>
parameter must define a single experience (the complete test set).
Defaults to False, which means that <code class="docutils literal notranslate"><span class="pre">train_list_of_files</span></code> and
<code class="docutils literal notranslate"><span class="pre">test_list_of_files</span></code> must contain the same amount of paths.</p></li>
<li><p><strong>train_transform</strong> – The transformation to apply to the training data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>train_target_transform</strong> – The transformation to apply to training
patterns targets. Defaults to None.</p></li>
<li><p><strong>eval_transform</strong> – The transformation to apply to the test data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>eval_target_transform</strong> – The transformation to apply to test
patterns targets. Defaults to None.</p></li>
<li><p><strong>other_streams_transforms</strong> – Transformations to apply to custom
streams. If no transformations are defined for a custom stream,
then “train” transformations will be used. This parameter must be a
dictionary mapping stream names to transformations. The transformations
must be a two elements tuple where the first element defines the
X transformation while the second element is the Y transformation.
Those elements can be None. If this dictionary contains the
transformations for “train” or “test” streams then those transformations
will override the <cite>train_transform</cite>, <cite>train_target_transform</cite>,
<cite>eval_transform</cite> and <cite>eval_target_transform</cite> parameters.</p></li>
<li><p><strong>dataset_type</strong> – The type of the dataset. Defaults to UNDEFINED.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">GenericCLScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_tensor_lists">
<code class="sig-prename descclassname"><span class="pre">avalanche.benchmarks.scenarios.generic_benchmark_creation.</span></code><code class="sig-name descname"><span class="pre">create_generic_benchmark_from_tensor_lists</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_tensors</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">task_labels</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complete_test_set_only</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">train_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eval_target_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">other_streams_transforms</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">,</span> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataset_type</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">AvalancheDatasetType</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">GenericCLScenario</span><a class="reference internal" href="../../../../../_modules/avalanche/benchmarks/scenarios/generic_benchmark_creation/#create_generic_benchmark_from_tensor_lists"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_generic_benchmark_from_tensor_lists" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a benchmark instance given lists of Tensors. A separate dataset will
be created from each Tensor tuple (x, y, z, …) and each of those training
datasets will be considered a separate training experience. Using this
helper function is the lowest-level way to create a Continual Learning
scenario. When possible, consider using higher level helpers.</p>
<p>Experiences are defined by passing lists of tensors as the <cite>train_tensors</cite>,
<cite>test_tensors</cite> (and <cite>other_streams_tensors</cite>) parameters. Those parameters
must be lists containing lists of tensors, one list for each experience.
Each tensor defines the value of a feature (“x”, “y”, “z”, …) for all
patterns of that experience.</p>
<p>By default the second tensor of each experience will be used to fill the
<cite>targets</cite> value (label of each pattern).</p>
<p>Beware that task labels can only be defined by choosing a single task label
for each experience (the same task label is applied to all patterns of
experiences sharing the same position in different streams).</p>
<p>When in need to create a benchmark instance in which task labels are defined
in a more fine-grained way, then consider using
<a class="reference internal" href="#avalanche.benchmarks.scenarios.generic_benchmark_creation.create_multi_dataset_generic_benchmark" title="avalanche.benchmarks.scenarios.generic_benchmark_creation.create_multi_dataset_generic_benchmark"><code class="xref py py-func docutils literal notranslate"><span class="pre">create_multi_dataset_generic_benchmark()</span></code></a> by passing properly
initialized <code class="xref py py-class docutils literal notranslate"><span class="pre">AvalancheDataset</span></code> instances.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_tensors</strong> – A list of lists. The first list must contain the
tensors for the first training experience (one tensor per feature), the
second list must contain the tensors for the second training experience,
and so on.</p></li>
<li><p><strong>test_tensors</strong> – A list of lists. The first list must contain the
tensors for the first test experience (one tensor per feature), the
second list must contain the tensors for the second test experience,
and so on. When using <cite>complete_test_set_only</cite>, this parameter
must be a list containing a single sub-list for the single test
experience.</p></li>
<li><p><strong>other_streams_tensors</strong> – A dictionary describing the content of
custom streams. Keys must be valid stream names (letters and numbers,
not starting with a number) while the value follow the same structure
of <cite>train_tensors</cite> and <cite>test_tensors</cite> parameters. If this
dictionary contains the definition for “train” or “test” streams then
those definition will  override the <cite>train_tensors</cite> and <cite>test_tensors</cite>
parameters.</p></li>
<li><p><strong>task_labels</strong> – A list of task labels. Must contain at least a value
for each experience. Each value describes the task label that will be
applied to all patterns of a certain experience. For more info on that,
see the function description.</p></li>
<li><p><strong>complete_test_set_only</strong> – If True, only the complete test set will
be returned by the scenario. This means that <code class="docutils literal notranslate"><span class="pre">test_tensors</span></code> must
define a single experience. Defaults to False, which means that
<code class="docutils literal notranslate"><span class="pre">train_tensors</span></code> and <code class="docutils literal notranslate"><span class="pre">test_tensors</span></code> must define the same
amount of experiences.</p></li>
<li><p><strong>train_transform</strong> – The transformation to apply to the training data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>train_target_transform</strong> – The transformation to apply to training
patterns targets. Defaults to None.</p></li>
<li><p><strong>eval_transform</strong> – The transformation to apply to the test data,
e.g. a random crop, a normalization or a concatenation of different
transformations (see torchvision.transform documentation for a
comprehensive list of possible transformations). Defaults to None.</p></li>
<li><p><strong>eval_target_transform</strong> – The transformation to apply to test
patterns targets. Defaults to None.</p></li>
<li><p><strong>other_streams_transforms</strong> – Transformations to apply to custom
streams. If no transformations are defined for a custom stream,
then “train” transformations will be used. This parameter must be a
dictionary mapping stream names to transformations. The transformations
must be a two elements tuple where the first element defines the
X transformation while the second element is the Y transformation.
Those elements can be None. If this dictionary contains the
transformations for “train” or “test” streams then those transformations
will override the <cite>train_transform</cite>, <cite>train_target_transform</cite>,
<cite>eval_transform</cite> and <cite>eval_target_transform</cite> parameters.</p></li>
<li><p><strong>dataset_type</strong> – The type of the dataset. Defaults to UNDEFINED.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A <code class="xref py py-class docutils literal notranslate"><span class="pre">GenericCLScenario</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

</div>
</div>
</div>


           </div>
           
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, ContinualAI.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>